{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d8a95e2-f4bb-4b11-9277-e607af4a3890",
   "metadata": {},
   "source": [
    "# homework 4 notebook:\n",
    "creating a logistic regression classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "869623aa-cc96-48af-80fb-e15385cb609f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the chapter on logistic regression, the book suggests a very small number\n",
    "# of features for classifying movie reviews\n",
    "# x1: count of positive words in the document\n",
    "# x2: count of negative words in the document\n",
    "# x3: 1 if \"no\" is in document, 0 otherwise\n",
    "# x4 count of first and second person pronouns\n",
    "# x5 1 if \"!\" is in document, 0 otherwise\n",
    "# x6 log(word count of document)\n",
    "\n",
    "# let's see if it works with Stochastic Gradient Descent\n",
    "import string\n",
    "\n",
    "from nltk.corpus import movie_reviews\n",
    "import random\n",
    "import nltk\n",
    "import math\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb07863e-558c-43b9-9bae-343783e8442b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6786 sentiment words.\n",
      "There are 2000 documents.\n"
     ]
    }
   ],
   "source": [
    "#the following block of code creates a dictionary of sentiment words whose value is 1 if it is a positive word\n",
    "# and 0 if it is a negative word.\n",
    "sentimentWordDictionary = {}\n",
    "f = open('positive-words.txt', 'r', encoding=\"latin-1\")\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    if len(line) == 0: # ignore this line\n",
    "        continue\n",
    "    if line[0] == ';': # ignore this line\n",
    "        continue\n",
    "    sentimentWordDictionary[line.lower()] = 1\n",
    "f.close()\n",
    "f = open('negative-words.txt', 'r', encoding=\"latin-1\")\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    if len(line) == 0: # ignore this line\n",
    "        continue\n",
    "    if line[0] == ';': # ignore this line\n",
    "        continue\n",
    "    sentimentWordDictionary[line.lower()] = 0\n",
    "f.close()\n",
    "\n",
    "# for debugging purposes\n",
    "print(\"There are\", len(sentimentWordDictionary), \"sentiment words.\")\n",
    "\n",
    "# Grab all the documents and shuffle them\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)\n",
    "print(\"There are\", len(documents), \"documents.\") # good for debugging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2018b32-69c3-459f-bf9d-c47d7791a0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function builds feature x1 and x2 by finding the num of positive and negative words in a certain document\n",
    "def countPositiveAndNegativeWords(d):\n",
    "    '''\n",
    "    Counts the number of positive and negative words in the document\n",
    "    :param d: A list containing the words in the document\n",
    "    :return: A tuple (positive, negative) with two integer values representing\n",
    "            the number of positive and negative words\n",
    "    '''\n",
    "    countPositive = 0\n",
    "    countNegative = 0\n",
    "    # iterate through every word in the document (bag of words)\n",
    "    for word in d:\n",
    "        # reference the dictionary containing the seniment for each word\n",
    "        # if the word is associated with a postive sentiment increment the count by 1\n",
    "        if word in sentimentWordDictionary:\n",
    "            if sentimentWordDictionary[word]== 1:\n",
    "                countPositive+=1\n",
    "            else:\n",
    "                countNegative+=1\n",
    "        \n",
    "    # you have to do this\n",
    "    return countPositive, countNegative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a71f96f5-8916-4d29-8b69-1c9f6d057fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function builds feature x3 for each document, checks if no is in the document \n",
    "def noInDocument(d):\n",
    "    '''\n",
    "    Returns 1 if the word \"no\" is in the document.   You may\n",
    "    want to contemplate whether you need to make this case sensitive or not.\n",
    "    :param d: A list of words in the document.\n",
    "    :return: 1 if no is in document; 0, otherwise.\n",
    "    '''\n",
    "    # convert all words to lowercase in the document first , will catch No and no\n",
    "    words = [word.lower() for word in d]\n",
    "\n",
    "    # check if the word no is in the list \n",
    "    if \"no\" in words:\n",
    "        return 1\n",
    "    # otherwise return 0 \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3425544a-4550-4e5d-a510-570b5418ee4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def countFirstSecondPersonPronouns(d):\n",
    "    '''\n",
    "    Returns a count of the number of first and second person pronouns\n",
    "    within the document.  You might want to look up what is a first or second\n",
    "    person pronoun.\n",
    "    :param d: A list of words in the document.\n",
    "    :return: The count of personal pronouns\n",
    "    '''\n",
    "    # create a list of first and second pronouns\n",
    "    count = 0\n",
    "    firstandsecondpronouns = [\"i\",\"we\",\"you\",\"me\",\"us\",\"my\",\"our\",\"your\",\"mine\",\"ours\",\"yours\",\"myself\",\"ourselves\",\"yourself\",\"yourself\"]\n",
    "     # convert all words to lowercase in the document first\n",
    "    words = [word.lower() for word in d]\n",
    "    for word in words:\n",
    "        if word in firstandsecondpronouns:\n",
    "            count+=1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59235127-d6cc-4c60-a402-76889d39c4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def exclamationInDocument(d):\n",
    "    '''\n",
    "    Returns 1 if the word \"!\" is in the document.\n",
    "    :param d: A list of words in the document.\n",
    "    :return: 1 if ! is in document; 0, otherwise.\n",
    "    '''\n",
    "    for word in d:\n",
    "        if word == \"!\":\n",
    "            return 1\n",
    "\n",
    "    # complete this\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5a29144-f451-4a81-a6ec-c6c1bc90f24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logOfLength(d):\n",
    "    '''\n",
    "    Computes and returns the log of the number of tokens in the document.\n",
    "    :param d: A list of words in the document.\n",
    "    :return: log(number of words)\n",
    "    '''\n",
    "    # complete this\n",
    "    # find the num of tokens \n",
    "    numtokens = len(d)\n",
    "    \n",
    "    return math.log(numtokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a1dfefba-b62c-4203-8cfe-475a1ab88f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(document):\n",
    "    '''\n",
    "    Builds the set of features for each document.\n",
    "    You don't need to modify this unless\n",
    "    you want to add another feature.\n",
    "    :param document: A list of words in the document.\n",
    "    :return: A dictionary containing the features for that document.\n",
    "    '''\n",
    "    document_words = list(document) # do not turn into a set!!\n",
    "    features = {}\n",
    "    positive, negative = countPositiveAndNegativeWords(document_words)\n",
    "\n",
    "    features['positiveCount']  = positive\n",
    "    features['negativeCount'] = negative\n",
    "    features['noInDoc'] = noInDocument(document_words)\n",
    "    features['personalPronounCount'] = countFirstSecondPersonPronouns(document_words)\n",
    "    features['exclamation'] = exclamationInDocument(document_words)\n",
    "    features['logLength'] = logOfLength(document_words)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "82ef6f56-ce98-43b2-aed0-e9aa1167d6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Fit 0.72\n"
     ]
    }
   ],
   "source": [
    "# for each document, extract its features\n",
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "\n",
    "# build the training and test sets\n",
    "trainingSize = int(0.8*len(featuresets))\n",
    "train_set, test_set = featuresets[0:trainingSize], featuresets[trainingSize:]\n",
    "\n",
    "# use stochastic gradient descent with log loss function\n",
    "classifier = LogisticRegression(max_iter=1000, verbose=0)\n",
    "x_Train = [list(a.values()) for (a,b) in train_set]\n",
    "y_Train = [b for (a,b) in train_set]\n",
    "classifier.fit(x_Train, y_Train)\n",
    "\n",
    "# print(classifier.coef_)  # if you want to see the coefficients, unsorted\n",
    "\n",
    "x_Test = [list(a.values()) for (a,b) in test_set]\n",
    "y_Test = [b for (a,b) in test_set]\n",
    "\n",
    "print(\"LR Fit\", classifier.score(x_Test, y_Test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6297fb0b-52ce-429e-9132-61a66560c878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('logLength', np.float64(0.6169818069093244))\n",
      "('exclamation', np.float64(-0.42969835122407063))\n",
      "('noInDoc', np.float64(-0.33406294331447434))\n",
      "('positiveCount', np.float64(0.10237283510207737))\n",
      "('negativeCount', np.float64(-0.08100240801434008))\n",
      "('personalPronounCount', np.float64(-0.025524583773895115))\n"
     ]
    }
   ],
   "source": [
    "#here is a block of code that sorts the features by absolute value\n",
    "# and prints them out\n",
    "featureNames = ['positiveCount', 'negativeCount', 'noInDoc', 'personalPronounCount',  'exclamation', 'logLength']\n",
    "featuresPlusImportance = [ (featureNames[i], classifier.coef_[0][i]) for i in range(len(classifier.coef_[0]))]\n",
    "featuresPlusImportance.sort(key = lambda x: abs(x[1]), reverse=True)\n",
    "for x in range(len(featuresPlusImportance)):\n",
    "    print(featuresPlusImportance[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "89b324bf-f108-4160-a315-91a5e1d6c1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    |   n   p |\n",
      "    |   e   o |\n",
      "    |   g   s |\n",
      "----+---------+\n",
      "neg |<137> 58 |\n",
      "pos |  54<151>|\n",
      "----+---------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "Tag | Prec.  | Recall | F-measure\n",
      "----+--------+--------+-----------\n",
      "neg | 0.7173 | 0.7026 | 0.7098\n",
      "pos | 0.7225 | 0.7366 | 0.7295\n",
      "\n"
     ]
    }
   ],
   "source": [
    "correct_tags = [c for (w, c) in test_set]\n",
    "test_tags = list(classifier.predict(x_Test))\n",
    "\n",
    "# how about its precision and recall per category\n",
    "mtrx = nltk.ConfusionMatrix(correct_tags, test_tags)\n",
    "print()\n",
    "print(mtrx)\n",
    "print()\n",
    "print(mtrx.evaluate())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7232c91-eeee-46a2-a8ad-f1b191c260c1",
   "metadata": {},
   "source": [
    "# Results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99397cd-7de0-4f28-8acf-8f46c6872e1f",
   "metadata": {},
   "source": [
    "The logistic regression classifier returned an accuracy score of 72%, meaning it used the fearures extracted to predict the sentiment correctly about 72% of the time. \\mThe classifier returned a precision score of about 72% for both negative and postive sentiment movies. \n",
    "\n",
    "This means when a movie was classified as neg, how many times was it actually negative , the true positives out of the all the classified positives. a movie was classified 137 times as a negative movie, with 54 false negatives. \n",
    "\n",
    "It was similar for positive movies with 151 correctly classfied as positive with 58 false positives. this is not ideal but also it does not seem like the model favor negative or positive classifactions over the other, meaning it is a relativley \"fair\" classifier. \n",
    "\n",
    "The recall score was 70% for negative senitment and ~74% for positive sentiment. so of all postivley sentimented movie reviews, the classifer caught 74% percent of them, and for all negative sentiment, the classifer caught 70% of them.\n",
    "\n",
    "\n",
    "The weights seem to make sense, I would have expected the positive and negative weights to be more important, but it could be that the words themselves individual may not be as important as the overall meaning, for example not and bad seperatley vs not bad, or not good. It is also interesting that lengthier reviews tend to be weighted more positivley by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "21104a54-edc9-4dbe-822d-2a690ebee199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rerunning the experiment with additional feauture: is a question mark present ? could be sarcasm or questioning a bad descision \n",
    "# novel feature x7: \n",
    "def questionMark(d):\n",
    "    '''\n",
    "    '''\n",
    "    return d.count(\"?\")    \n",
    "    # check if question mark is present \n",
    "    for w in d:\n",
    "        if w == \"?\":\n",
    "            return 1\n",
    "    return 0\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ebb6dff4-773a-4838-9685-3fb5d8beac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the feature to the feature extractor\n",
    "def document_features(document):\n",
    "    '''\n",
    "    Builds the set of features for each document.\n",
    "    You don't need to modify this unless\n",
    "    you want to add another feature.\n",
    "    :param document: A list of words in the document.\n",
    "    :return: A dictionary containing the features for that document.\n",
    "    '''\n",
    "    document_words = list(document) # do not turn into a set!!\n",
    "    features = {}\n",
    "    positive, negative = countPositiveAndNegativeWords(document_words)\n",
    "\n",
    "    features['positiveCount']  = positive\n",
    "    features['negativeCount'] = negative\n",
    "    features['noInDoc'] = noInDocument(document_words)\n",
    "    features['personalPronounCount'] = countFirstSecondPersonPronouns(document_words)\n",
    "    features['exclamation'] = exclamationInDocument(document_words)\n",
    "    features['logLength'] = logOfLength(document_words)\n",
    "    features['Questionmark'] = questionMark(document_words)\n",
    "    \n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5621a911-0bd7-4d6d-bd4e-a2f4411c3d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Fit 0.7175\n"
     ]
    }
   ],
   "source": [
    "# for each document, extract its features\n",
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "\n",
    "# build the training and test sets\n",
    "trainingSize = int(0.8*len(featuresets))\n",
    "train_set, test_set = featuresets[0:trainingSize], featuresets[trainingSize:]\n",
    "\n",
    "# use stochastic gradient descent with log loss function\n",
    "classifier = LogisticRegression(max_iter=1000, verbose=0)\n",
    "x_Train = [list(a.values()) for (a,b) in train_set]\n",
    "y_Train = [b for (a,b) in train_set]\n",
    "classifier.fit(x_Train, y_Train)\n",
    "\n",
    "# print(classifier.coef_)  # if you want to see the coefficients, unsorted\n",
    "\n",
    "x_Test = [list(a.values()) for (a,b) in test_set]\n",
    "y_Test = [b for (a,b) in test_set]\n",
    "\n",
    "print(\"LR Fit\", classifier.score(x_Test, y_Test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e15ca6a4-3761-455c-a445-01d99e48ed35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('logLength', np.float64(0.6255510802576381))\n",
      "('exclamation', np.float64(-0.40268747809956323))\n",
      "('noInDoc', np.float64(-0.33105361857492815))\n",
      "('positiveCount', np.float64(0.10220807934485596))\n",
      "('negativeCount', np.float64(-0.08026400217535848))\n",
      "('Questionmark', np.float64(-0.03929857442172685))\n",
      "('personalPronounCount', np.float64(-0.02222245467601263))\n"
     ]
    }
   ],
   "source": [
    "#here is a block of code that sorts the features by absolute value\n",
    "# and prints them out\n",
    "featureNames = ['positiveCount', 'negativeCount', 'noInDoc', 'personalPronounCount',  'exclamation', 'logLength','Questionmark']\n",
    "featuresPlusImportance = [ (featureNames[i], classifier.coef_[0][i]) for i in range(len(classifier.coef_[0]))]\n",
    "featuresPlusImportance.sort(key = lambda x: abs(x[1]), reverse=True)\n",
    "for x in range(len(featuresPlusImportance)):\n",
    "    print(featuresPlusImportance[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7a83af10-cb88-4d75-a719-c11483be7e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    |   n   p |\n",
      "    |   e   o |\n",
      "    |   g   s |\n",
      "----+---------+\n",
      "neg |<137> 58 |\n",
      "pos |  55<150>|\n",
      "----+---------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "Tag | Prec.  | Recall | F-measure\n",
      "----+--------+--------+-----------\n",
      "neg | 0.7135 | 0.7026 | 0.7080\n",
      "pos | 0.7212 | 0.7317 | 0.7264\n",
      "\n"
     ]
    }
   ],
   "source": [
    "correct_tags = [c for (w, c) in test_set]\n",
    "test_tags = list(classifier.predict(x_Test))\n",
    "\n",
    "# how about its precision and recall per category\n",
    "mtrx = nltk.ConfusionMatrix(correct_tags, test_tags)\n",
    "print()\n",
    "print(mtrx)\n",
    "print()\n",
    "print(mtrx.evaluate())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ba9409-6fb1-4932-8f43-329d85972abe",
   "metadata": {},
   "source": [
    "# New feature: question mark\n",
    "new feature (x8) checks whether a question mark is in the document. it returns 1 if it does, 0 otherwise.\n",
    "I did this to see if it would capture rhetorical questions and sarcasm, or a critical tone.\n",
    "\n",
    "\n",
    "After rerunning the model training and testing i got a confusion matrix\n",
    "\n",
    "    |   n   p |\n",
    "    |   e   o |\n",
    "    |   g   s |\n",
    "----+---------+\n",
    "neg |<137> 58 |\n",
    "pos |  55<150>|\n",
    "----+---------+\n",
    "(row = reference; col = test)\n",
    "\n",
    "Compared to the previous model the matrix did not change significantly but they did shift a bit. The feature was weighted with a small negative number: -0.039, this means it only had a minor influence on classification, and reviews with question marks had a small tendancy to be negative. Positive classifaction was reduced by 1 . Precision, recall and f1 score did not change significantly. This may because question marks only occur in rare instances and so the feature matrix for it is sparse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449f93d8-6ae9-450e-ae52-443e25648299",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
