{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70d54941-e282-416e-a727-66f5b2b8c583",
   "metadata": {},
   "source": [
    "# homework 2 \n",
    "## Matthew Martin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2160b141-1d9f-465e-88ba-c3321b012afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")   # main tokenizer model\n",
    "nltk.download(\"punkt_tab\")  # sometimes needed in newer NLTK versions\n",
    "import re\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29f51e4-c523-44e5-9917-f4e79118a0f4",
   "metadata": {},
   "source": [
    "# Problem 1: Warm up excercise\n",
    "\n",
    "Rewrite the following loop as a list comprehension:\n",
    "\n",
    " \n",
    "\n",
    "sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']\n",
    "\n",
    "result = []\n",
    "\n",
    "for word in sent:\n",
    "\n",
    "...     word_len = (word, len(word))\n",
    "\n",
    "...     result.append(word_len)\n",
    "\n",
    " result\n",
    "\n",
    "[('The', 3), ('dog', 3), ('gave', 4), ('John', 4), ('the', 3), ('newspaper', 9)]\n",
    "\n",
    "Please put this code (and whatever you use to test it) in a file called Problem1.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c300377-c3b4-4fbb-8f21-2ba5f905462f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 3),\n",
       " ('dog', 3),\n",
       " ('gave', 4),\n",
       " ('John', 4),\n",
       " ('the', 3),\n",
       " ('newspaper', 9)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# declare the iterable \n",
    "sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']\n",
    "\n",
    "# use list comprehension to iterate trhough \n",
    "result = [(word,len(word)) for word in sent]\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbb1c1a-ed90-4a75-a4d0-e3a000992ee3",
   "metadata": {},
   "source": [
    "# Problem 2: BPE \n",
    "perform byte pair encoding on Dorain Gray text\n",
    "\n",
    "A) how many tokens are in the file \n",
    "B) how manty unique tokens are in the file \n",
    "c) assunming we set the parameter vocab_size = 2000 during training , how many non-unique BPE encodings are needed ? \n",
    "d) if that text is encoded how many unique BPE encodings are needed? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "57f52979-16c1-46c6-8431-a40c6afa2fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dorian gray file first\n",
    "f = open('DorianGray.txt')\n",
    "# create a string that stores all the text\n",
    "raw = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f0c4ffb4-6d2f-4e15-84ee-6430eed0fd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount of tokens in Dorain Gray:  98399\n",
      "amount of unique tokens in Dorain Gray:  8626\n"
     ]
    }
   ],
   "source": [
    "# tokenize the texts into tokens( basic unit you get when you split text into smaller pieces) \n",
    "tokens = nltk.word_tokenize(raw)\n",
    "# calculate how many tokens are in the dorian gray text\n",
    "print(\"amount of tokens in Dorain Gray: \",len(tokens))\n",
    "print(\"amount of unique tokens in Dorain Gray: \",len(set(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9ccca2d7-b3f8-422b-8094-4eaecf5a039f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: DorianGray.txt\n",
      "  input_format: \n",
      "  model_prefix: bpe\n",
      "  model_type: BPE\n",
      "  vocab_size: 2000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: <eos>\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(186) LOG(INFO) Loading corpus: DorianGray.txt\n",
      "trainer_interface.cc(411) LOG(INFO) Loaded all 8906 sentences\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <eos>\n",
      "trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(541) LOG(INFO) all chars count=446682\n",
      "trainer_interface.cc(552) LOG(INFO) Done: 99.953% characters are covered.\n",
      "trainer_interface.cc(562) LOG(INFO) Alphabet size=65\n",
      "trainer_interface.cc(563) LOG(INFO) Final character coverage=0.99953\n",
      "trainer_interface.cc(594) LOG(INFO) Done! preprocessed 7503 sentences.\n",
      "trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 7503\n",
      "trainer_interface.cc(611) LOG(INFO) Done! 12539\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=10569 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2667 size=20 all=1613 active=1547 piece=en\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1762 size=40 all=2150 active=2084 piece=▁in\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1066 size=60 all=2695 active=2629 piece=ld\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=770 size=80 all=3327 active=3261 piece=al\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=619 size=100 all=3810 active=3744 piece=ry\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=619 min_freq=35\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=456 size=120 all=4308 active=1471 piece=ter\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=380 size=140 all=4700 active=1863 piece=ord\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=315 size=160 all=5085 active=2248 piece=qu\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=283 size=180 all=5373 active=2536 piece=ess\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=253 size=200 all=5671 active=2834 piece=▁up\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=253 min_freq=32\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=229 size=220 all=5873 active=1198 piece=▁cha\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=209 size=240 all=6173 active=1498 piece=ass\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=188 size=260 all=6480 active=1805 piece=▁there\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=163 size=280 all=6820 active=2145 piece=▁ta\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=153 size=300 all=7080 active=2405 piece=ought\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=153 min_freq=28\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=141 size=320 all=7308 active=1219 piece=▁should\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=133 size=340 all=7508 active=1419 piece=▁too\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=126 size=360 all=7732 active=1643 piece=ence\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=116 size=380 all=7934 active=1845 piece=ject\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=109 size=400 all=8122 active=2033 piece=▁down\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=109 min_freq=25\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=104 size=420 all=8279 active=1153 piece=▁did\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=97 size=440 all=8447 active=1321 piece=▁mom\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=92 size=460 all=8516 active=1390 piece=hy\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=87 size=480 all=8637 active=1511 piece=▁going\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=82 size=500 all=8789 active=1663 piece=▁how\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=82 min_freq=22\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=78 size=520 all=8899 active=1110 piece=▁young\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=74 size=540 all=9135 active=1346 piece=let\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=71 size=560 all=9256 active=1467 piece=▁dear\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=67 size=580 all=9362 active=1573 piece=▁anything\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=63 size=600 all=9530 active=1741 piece=ied\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=63 min_freq=20\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=61 size=620 all=9692 active=1148 piece=▁asked\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=59 size=640 all=9734 active=1190 piece=▁Lady\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=56 size=660 all=9858 active=1314 piece=▁lips\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=54 size=680 all=9970 active=1426 piece=▁got\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=53 size=700 all=10000 active=1456 piece=▁passion\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=53 min_freq=18\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=50 size=720 all=10122 active=1122 piece=kes\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=49 size=740 all=10245 active=1245 piece=▁true\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=47 size=760 all=10345 active=1345 piece=▁mor\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=46 size=780 all=10445 active=1445 piece=▁sub\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=44 size=800 all=10501 active=1501 piece=▁wind\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=44 min_freq=17\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=42 size=820 all=10549 active=1047 piece=not\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=41 size=840 all=10644 active=1142 piece=▁fan\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=40 size=860 all=10710 active=1208 piece=▁take\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=39 size=880 all=10762 active=1260 piece=▁forget\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=38 size=900 all=10862 active=1360 piece=▁dress\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=38 min_freq=15\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=37 size=920 all=10942 active=1077 piece=▁each\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=35 size=940 all=11020 active=1155 piece=ool\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=35 size=960 all=11069 active=1204 piece=▁without\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=34 size=980 all=11118 active=1253 piece=▁hideous\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=32 size=1000 all=11259 active=1394 piece=vel\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=32 min_freq=14\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=31 size=1020 all=11318 active=1045 piece=▁To\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=30 size=1040 all=11395 active=1122 piece=▁hear\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=29 size=1060 all=11468 active=1195 piece=selves\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=28 size=1080 all=11548 active=1275 piece=ared\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=28 size=1100 all=11600 active=1327 piece=▁certainly\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=28 min_freq=13\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=27 size=1120 all=11688 active=1089 piece=▁why\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=26 size=1140 all=11731 active=1132 piece=med\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=26 size=1160 all=11777 active=1178 piece=esides\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=25 size=1180 all=11823 active=1224 piece=▁ru\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=25 size=1200 all=11862 active=1263 piece=▁fingers\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=25 min_freq=12\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=24 size=1220 all=11940 active=1079 piece=▁far\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=24 size=1240 all=11976 active=1115 piece=▁silver\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=23 size=1260 all=12060 active=1199 piece=land\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=23 size=1280 all=12114 active=1253 piece=▁shame\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=22 size=1300 all=12148 active=1287 piece=gar\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=22 min_freq=11\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=22 size=1320 all=12230 active=1076 piece=▁Some\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=22 size=1340 all=12242 active=1088 piece=borough\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=21 size=1360 all=12308 active=1154 piece=aint\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=21 size=1380 all=12378 active=1224 piece=▁given\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=20 size=1400 all=12402 active=1248 piece=▁gu\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=20 min_freq=10\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=20 size=1420 all=12443 active=1035 piece=▁open\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=20 size=1440 all=12452 active=1044 piece=▁remembered\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=19 size=1460 all=12552 active=1144 piece=▁low\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=19 size=1480 all=12585 active=1177 piece=▁wrong\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18 size=1500 all=12603 active=1195 piece=ank\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=18 min_freq=9\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18 size=1520 all=12683 active=1072 piece=▁reg\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18 size=1540 all=12720 active=1109 piece=▁bring\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18 size=1560 all=12717 active=1106 piece=▁vulgar\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17 size=1580 all=12774 active=1163 piece=hion\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17 size=1600 all=12845 active=1234 piece=▁five\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=17 min_freq=9\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17 size=1620 all=12855 active=1011 piece=▁wanted\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=1640 all=12872 active=1028 piece=ier\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=1660 all=12952 active=1108 piece=Harry\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=1680 all=12963 active=1119 piece=▁bright\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=1700 all=12976 active=1132 piece=ail\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=15 min_freq=8\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=1720 all=13063 active=1080 piece=▁Jim\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=1740 all=13116 active=1133 piece=rming\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=1760 all=13144 active=1161 piece=▁compl\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=1780 all=13141 active=1158 piece=▁played\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=1800 all=13149 active=1166 piece=be\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=14 min_freq=8\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=1820 all=13247 active=1088 piece=▁ef\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=1840 all=13291 active=1132 piece=▁lim\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=1860 all=13328 active=1169 piece=agraph\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=1880 all=13345 active=1186 piece=▁silent\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=1900 all=13372 active=1213 piece=na\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=13 min_freq=7\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=1920 all=13478 active=1095 piece=lled\n",
      "trainer_interface.cc(689) LOG(INFO) Saving model: bpe.model\n",
      "trainer_interface.cc(701) LOG(INFO) Saving vocabs: bpe.vocab\n"
     ]
    }
   ],
   "source": [
    "# train the BPE model on the text\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=\"DorianGray.txt\", # this is the text it uses to build the BPE encoding, could be any file of text\n",
    "    model_prefix=\"bpe\",  # this will create a model called \"bpe.model\"\n",
    "    vocab_size=2000, # this needs to be adjusted somewhat depending on the size of the input text and the number of unique characters\n",
    "    model_type=\"bpe\", # use Byte-Pair Encoding\n",
    "    user_defined_symbols=[\"<eos>\"], # marker for end-of-sentence\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "05fdadfc-44fa-4aec-911e-179f92619a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and use the model\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"bpe.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "24d64b29-8bf7-4e1d-9464-ebb2c838cdf7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we want to know how many non unique bpe encodings are needed \n",
    "\n",
    "#encode dorian gray \n",
    "tokens = sp.encode(raw,out_type = str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "39e1a5dd-8282-4203-9481-4b1300f016e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129323\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cd89c67f-5101-44e6-9534-ca0c53236787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2006"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many unique BPE encodings are needed \n",
    "len(set(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd83914-4184-47b6-b36f-fefef925d281",
   "metadata": {},
   "source": [
    "# problem 3: Minimum Edit distance\n",
    "a) Verify that the cost of the edit distance between \"INTENTION\" and \"EXECUTION\" is the same as given in the Jurafsky and Martin textbook.  You may have to pass an extra parameter to use the same cost for substitutions that Jurafsky and Martin use.  This is doing a character-by-character distance.   \n",
    "\n",
    "b) Instead of doing a character-by-character distance, let's do a word-by-word distance.  Use the function to determine the edit distance between words in the two sentences: \"The girl hit the ball\" and \"The girl danced at the ball\".  You'll have to have your python code convert those strings into lists of words.  What NLTK function could you use to do that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ba637bf5-66cd-449e-8077-24a2eb1100af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# char by char distance , part A\n",
    "from nltk.metrics.distance import edit_distance\n",
    "intention = \"INTENTION\"\n",
    "execution = \"EXECUTION\"\n",
    "distance = edit_distance(intention, execution, substitution_cost=2)\n",
    "distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5ce91a8b-eb90-45c3-9035-cc81ba07f9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word by word , Part B\n",
    "s1 = \"The girl hit the ball\"\n",
    "s2 = \"The girl danced at the ball\"\n",
    "# convert strings into a list of words using nltk , turn it into text \n",
    "s1 = nltk.word_tokenize(s1)\n",
    "s2 = nltk.word_tokenize(s2)\n",
    "# calculate the distance should be 3-  one subsitution and one insertion \n",
    "distance = edit_distance(s1, s2, substitution_cost=2)\n",
    "distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d040f8-494c-4733-a964-1386a247faeb",
   "metadata": {},
   "source": [
    "# Problem 4: N-Grams Exercise\n",
    "A)What are the five most frequent bigrams in that text?\n",
    "\n",
    "b) What are the five most common words that come after the full word \"the\" (including \"The\")?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ad57f29c-635f-4a5a-9a9e-bc494145b010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((',', 'and'), 945), (('”', '“'), 723), (('.', '“'), 668), (('.', 'I'), 475), (('of', 'the'), 429)]\n"
     ]
    }
   ],
   "source": [
    "# take the raw text and tokenize it \n",
    "# import the dorian gray file first\n",
    "f = open('DorianGray.txt')\n",
    "# create a string that stores all the text\n",
    "raw = f.read()\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "# compute all the bigrams and convert to list \n",
    "all_bigrams = list(nltk.bigrams(tokens))\n",
    "# claculate the freq dist and take top five \n",
    "dist = nltk.FreqDist(all_bigrams)\n",
    "topfive = dist.most_common(5)\n",
    "print(topfive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "67bd237c-d532-4628-944b-c4f09756fb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('the', 'world'), 64), (('the', 'room'), 58), (('the', 'door'), 55), (('the', 'man'), 42), (('the', 'table'), 41)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['world', 'room', 'door', 'man', 'table']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most common words that follow the or The ? \n",
    "the = [words for words in all_bigrams if words[0].lower() == \"the\"]\n",
    "# create a freq dist for these bigrams to see which ioccur most freq\n",
    "thedist = nltk.FreqDist(the)\n",
    "# select the top 5 miost freq \n",
    "thetopfive = thedist.most_common(5)\n",
    "print(thetopfive)\n",
    "secondwords = [bigram[0][1] for bigram in thetopfive]\n",
    "secondwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7e70ed-9c89-4fce-98e6-12cb5cd89b33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
